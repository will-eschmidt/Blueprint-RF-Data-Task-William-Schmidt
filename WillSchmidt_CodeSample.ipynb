{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Case Study 1</h1></center>\n",
    "<center><h3> Week 1 (out of 5)</h3></center>\n",
    "\n",
    "**Author of this file:**\n",
    "1. Will Schmidt (will.schmidt@emory.edu)\n",
    " \n",
    "**Data Source**: W.C. Hunter and M.B. Walker (1996), [“*The Cultural Affinity Hypothesis and Mortgage Lending Decisions*,”](https://link.springer.com/article/10.1007/BF00174551) Journal of Real Estate Finance and Economics 13, 57-70.\n",
    " \n",
    "**Book**: [Introductory Econometrics: A Modern Approach](https://economics.ut.ac.ir/documents/3030266/14100645/Jeffrey_M._Wooldridge_Introductory_Econometrics_A_Modern_Approach__2012.pdf) by Jeffrey Wooldridge\n",
    "\n",
    "**Data Description**: ```http://fmwww.bc.edu/ec-p/data/wooldridge/loanapp.dta```\n",
    "\n",
    "```\n",
    "  Obs:  1989\n",
    "\n",
    "  1. occ                       occupancy\n",
    "  2. loanamt                   loan amt in thousands\n",
    "  3. action                    type of action taken\n",
    "  4. msa                       msa number of property\n",
    "  5. suffolk                   =1 if property in Suffolk County\n",
    "  6. race                      race of applicant\n",
    "  7. gender                    gender of applicant\n",
    "  8. appinc                    applicant income, $1000s\n",
    "  9. typur                     type of purchaser of loan\n",
    " 10. unit                      number of units in property\n",
    " 11. married                   =1 if applicant married\n",
    " 12. dep                       number of dependents\n",
    " 13. emp                       years employed in line of work\n",
    " 14. yjob                      years at this job\n",
    " 15. self                      self-employment dummy\n",
    " 16. atotinc                   total monthly income\n",
    " 17. cototinc                  coapp total monthly income\n",
    " 18. hexp                      propose housing expense\n",
    " 19. price                     purchase price\n",
    " 20. other                     other financing, $1000s\n",
    " 21. liq                       liquid assets\n",
    " 22. rep                       no. of credit reports\n",
    " 23. gdlin                     credit history meets guidelines\n",
    " 24. lines                     no. of credit lines on reports\n",
    " 25. mortg                     credit history on mortgage paym\n",
    " 26. cons                      credit history on consumer stuf\n",
    " 27. pubrec                    =1 if filed bankruptcy\n",
    " 28. hrat                      housing exp, % total inccome\n",
    " 29. obrat                     other oblgs,  % total income\n",
    " 30. fixadj                    fixed or adjustable rate?\n",
    " 31. term                      term of loan in months\n",
    " 32. apr                       appraised value\n",
    " 33. prop                      type of property\n",
    " 34. inss                      PMI sought\n",
    " 35. inson                     PMI approved\n",
    " 36. gift                      gift as down payment\n",
    " 37. cosign                    is there a cosigner\n",
    " 38. unver                     unverifiable info\n",
    " 39. review                    number of times reviewed\n",
    " 40. netw                      net worth\n",
    " 41. unem                      unemployment rate by industry\n",
    " 42. min30                     =1 if minority pop. > 30%\n",
    " 43. bd                        =1 if boarded-up val > MSA med\n",
    " 44. mi                        =1 if tract inc > MSA median\n",
    " 45. old                       =1 if applic age > MSA median\n",
    " 46. vr                        =1 if tract vac rte > MSA med\n",
    " 47. sch                       =1 if > 12 years schooling\n",
    " 48. black                     =1 if applicant black\n",
    " 49. hispan                    =1 if applicant Hispanic\n",
    " 50. male                      =1 if applicant male\n",
    " 51. reject                    =1 if action == 3\n",
    " 52. approve                   =1 if action == 1 or 2\n",
    " 53. mortno                    no mortgage history\n",
    " 54. mortperf                  no late mort. payments\n",
    " 55. mortlat1                  one or two late payments\n",
    " 56. mortlat2                  > 2 late payments\n",
    " 57. chist                     =0 if accnts deliq. >= 60 days\n",
    " 58. multi                     =1 if two or more units\n",
    " 59. loanprc                   amt/price\n",
    " 60. thick                     =1 if rep > 2\n",
    " 61. white                     =1 if applicant white\n",
    " 62. obwhte                    obrat*awhite\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Questions</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the 62 variables in the data set create a new data frame containing _all_ the 26 explanatory variables described in Table 1 (page 61) as well as the outcome variable `action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary data\n",
    "import pandas as pd\n",
    "df = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/loanapp.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data frame is (1989, 27).\n",
      "The columns are ['APPROVE', 'HRAT', 'OBRAT', 'MHIST', 'PUB', 'SELF', 'CHIST', 'UEMP', 'MULTI', 'COSIGN', 'MS', 'LNPR', 'DEP', 'SCH', 'THK', 'ARAC', 'ASEX', 'LOC', 'SCHRAC', 'ACHSEX', 'SCHTHK', 'SCHELOC', 'CHRAC', 'CHSEX', 'SHTHK', 'CHLOC', 'OBRAC'].\n"
     ]
    }
   ],
   "source": [
    "# match varible names in the paper and dataset\n",
    "# according to the definition in Table 1, pub = pubrec\n",
    "# however, based on the summary statistics in Table 2, pub = 1 - pubrec\n",
    "# Therefore, I adjust pub = 1 - pubrec. Similarly for Morhist and location \n",
    "\n",
    "# uemp = unem\n",
    "# ms = married\n",
    "# lnpr = loanprc\n",
    "# thk = thick\n",
    "# asex = male\n",
    "# arac = white\n",
    "df['pub'] = 1 - df['pubrec']\n",
    "df['mhist'] = 1 - df['mortlat2']\n",
    "df['loc'] = 1 - df['vr']\n",
    "\n",
    "var = ['approve', 'hrat', 'obrat', 'mhist', 'pub', 'self', 'chist', 'unem', 'multi', 'cosign', 'married',\n",
    "      'loanprc', 'dep', 'sch', 'thick', 'white', 'male', 'loc']\n",
    "\n",
    "#create our interaction terms\n",
    "tmp = ['white', 'male', 'thick', 'loc']\n",
    "for x in tmp:\n",
    "    df['sch'+x] = df['sch']*df[x]\n",
    "    var.append('sch' + x)\n",
    "for x in tmp:\n",
    "    df['chist'+x] = df['chist']*df[x]\n",
    "    var.append('chist' + x)\n",
    "\n",
    "# obrac = obwhte\n",
    "var.append('obwhte') # var is now a complete list of names of all variables we need in df\n",
    "df_new = df[var]\n",
    "\n",
    "# change variable names as in the paper\n",
    "varname = ['approve', 'hrat', 'obrat', 'mhist', 'pub', 'self', 'chist', 'uemp', 'multi', 'cosign', 'ms', \n",
    "           'lnpr', 'dep', 'sch', 'thk', 'arac', 'asex', 'loc', 'schrac','achsex','schthk','scheloc','chrac',\n",
    "           'chsex','shthk','chloc','obrac']\n",
    "varname = list(map(lambda x:x.upper(), varname)) # upper case\n",
    "df_new.columns = varname\n",
    "\n",
    "# display the shape and columns names of df_new to indicate it is the data frame we want\n",
    "print(f'The shape of the data frame is {df_new.shape}.')\n",
    "print(f'The columns are {list(df_new.columns)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After reading the original article try to _replicate_ Table 2 (page 62) using the 1989 observations. **Note**: The numbers will not be _exactly_ the same because the provided data set is missing 2 observations from the original 1,991 observations used in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2. Summary statistics for variables in model. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable Name</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>HRAT</td>\n",
       "      <td>24.791</td>\n",
       "      <td>7.119</td>\n",
       "      <td>1.000</td>\n",
       "      <td>72.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>OBRAT</td>\n",
       "      <td>32.389</td>\n",
       "      <td>8.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MHIST</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>PUB</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SELF</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHIST</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>UEMP</td>\n",
       "      <td>3.882</td>\n",
       "      <td>2.164</td>\n",
       "      <td>1.800</td>\n",
       "      <td>10.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MULTI</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>COSIGN</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MS</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>LNPR</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>DEP</td>\n",
       "      <td>0.771</td>\n",
       "      <td>1.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCH</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>THK</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ARAC</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ASEX</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCHRAC</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ACHSEX</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCHTHK</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCHELOC</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHRAC</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHSEX</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SHTHK</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHLOC</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>OBRAC</td>\n",
       "      <td>27.061</td>\n",
       "      <td>13.828</td>\n",
       "      <td>0.000</td>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       " Variable Name    Mean  Standard Deviation  Minimum  Maximum\n",
       "          HRAT  24.791               7.119    1.000   72.000\n",
       "         OBRAT  32.389               8.263    0.000   95.000\n",
       "         MHIST   0.989               0.102    0.000    1.000\n",
       "           PUB   0.931               0.253    0.000    1.000\n",
       "          SELF   0.129               0.336    0.000    1.000\n",
       "         CHIST   0.838               0.369    0.000    1.000\n",
       "          UEMP   3.882               2.164    1.800   10.600\n",
       "         MULTI   0.086               0.281    0.000    1.000\n",
       "        COSIGN   0.029               0.167    0.000    1.000\n",
       "            MS   0.659               0.474    0.000    1.000\n",
       "          LNPR   0.771               0.189    0.021    2.571\n",
       "           DEP   0.771               1.104    0.000    8.000\n",
       "           SCH   0.772               0.420    0.000    1.000\n",
       "           THK   0.105               0.307    0.000    1.000\n",
       "          ARAC   0.845               0.362    0.000    1.000\n",
       "          ASEX   0.813               0.390    0.000    1.000\n",
       "           LOC   0.590               0.492    0.000    1.000\n",
       "        SCHRAC   0.668               0.471    0.000    1.000\n",
       "        ACHSEX   0.620               0.485    0.000    1.000\n",
       "        SCHTHK   0.083               0.276    0.000    1.000\n",
       "       SCHELOC   0.465               0.499    0.000    1.000\n",
       "         CHRAC   0.727               0.446    0.000    1.000\n",
       "         CHSEX   0.680               0.467    0.000    1.000\n",
       "         SHTHK   0.083               0.276    0.000    1.000\n",
       "         CHLOC   0.497               0.500    0.000    1.000\n",
       "         OBRAC  27.061              13.828    0.000   95.000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate stats and transpose\n",
    "t2 = df_new.describe().loc[['mean','std','min','max']].transpose().iloc[1:]\n",
    "t2.reset_index(inplace = True)\n",
    "\n",
    "# reset column names \n",
    "colname = ['Variable Name', 'Mean', 'Standard Deviation', 'Minimum', 'Maximum']\n",
    "t2.columns = colname\n",
    "\n",
    "# make index blank\n",
    "t2.index = ['']*t2.shape[0]\n",
    "\n",
    "t2 = t2.round(3)\n",
    "print('Table 2. Summary statistics for variables in model. ')\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replicate the _Estimated Coefficient_ column of Table 3 (page 63). You are also expected to print the _Variable Name_ column. **Note:** You are strongly advised to use the [Logit](https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit) command from the `statsmodels` library to answer this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.299449\n",
      "         Iterations 7\n",
      "Table 3. Logit results on mortgage acceptances\n",
      "(dependent variable = 1 if mortgage was accepted, 0 if mortgage was denied). \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable Name</th>\n",
       "      <th>Estimated Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>HRAT</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>OBRAT</td>\n",
       "      <td>-0.0662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MHIST</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>PUB</td>\n",
       "      <td>-1.3870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SELF</td>\n",
       "      <td>-0.5550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHIST</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>UEMP</td>\n",
       "      <td>-0.0530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MULTI</td>\n",
       "      <td>-0.5198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>COSIGN</td>\n",
       "      <td>0.3572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MS</td>\n",
       "      <td>0.4683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>LNPR</td>\n",
       "      <td>-1.8652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>DEP</td>\n",
       "      <td>-0.0837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCH</td>\n",
       "      <td>-0.8557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>THK</td>\n",
       "      <td>-0.4967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ARAC</td>\n",
       "      <td>0.5939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ASEX</td>\n",
       "      <td>-0.7405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>LOC</td>\n",
       "      <td>-0.3427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCHRAC</td>\n",
       "      <td>-0.3766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ACHSEX</td>\n",
       "      <td>0.9299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCHTHK</td>\n",
       "      <td>0.7567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SCHELOC</td>\n",
       "      <td>0.4327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHRAC</td>\n",
       "      <td>-0.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHSEX</td>\n",
       "      <td>-0.1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>SHTHK</td>\n",
       "      <td>-0.0814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CHLOC</td>\n",
       "      <td>0.4945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>OBRAC</td>\n",
       "      <td>0.0185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       " Variable Name Estimated Coefficient\n",
       "          HRAT                0.0127\n",
       "         OBRAT               -0.0662\n",
       "         MHIST                0.7371\n",
       "           PUB               -1.3870\n",
       "          SELF               -0.5550\n",
       "         CHIST                0.9179\n",
       "          UEMP               -0.0530\n",
       "         MULTI               -0.5198\n",
       "        COSIGN                0.3572\n",
       "            MS                0.4683\n",
       "          LNPR               -1.8652\n",
       "           DEP               -0.0837\n",
       "           SCH               -0.8557\n",
       "           THK               -0.4967\n",
       "          ARAC                0.5939\n",
       "          ASEX               -0.7405\n",
       "           LOC               -0.3427\n",
       "        SCHRAC               -0.3766\n",
       "        ACHSEX                0.9299\n",
       "        SCHTHK                0.7567\n",
       "       SCHELOC                0.4327\n",
       "         CHRAC               -0.0590\n",
       "         CHSEX               -0.1104\n",
       "         SHTHK               -0.0814\n",
       "         CHLOC                0.4945\n",
       "         OBRAC                0.0185"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "df_new = df_new.dropna()\n",
    "X = df_new.drop('APPROVE', axis = 1)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# according to table3 in the paper, pub = pubrec\n",
    "X['PUB'] = 1-X['PUB']\n",
    "y = df_new[['APPROVE']]\n",
    "reg = sm.Logit(y, X).fit()\n",
    "\n",
    "coef = pd.DataFrame(reg.summary().tables[1]).iloc[2:, 1]\n",
    "t3 = pd.DataFrame({\n",
    "    'Variable Name': varname[1:],\n",
    "    'Estimated Coefficient': coef\n",
    "})\n",
    "\n",
    "# make index blank\n",
    "t3.index = ['']*t3.shape[0]\n",
    "\n",
    "print('Table 3. Logit results on mortgage acceptances\\n(dependent variable = 1 if mortgage was accepted, 0 if mortgage was denied). ')\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. After partitioning the original 1,989 observations into a training data set (80%) and validation set (20%) using a `random_state` equal to 42, proceed to train the model you fit in 3 above using the `LogisticRegression` function from the `sklearn` library with the training data set and then print the proportion of true predictions for your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = X.drop('const', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of true predictions for the validation set is 0.8829516539440203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "logit1 = LogisticRegression(fit_intercept=True,max_iter=1000,solver='lbfgs',penalty='none').fit(X_train, y_train.values.ravel())\n",
    "print('The proportion of true predictions for the validation set is', logit1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Can you _build_ a better model? Using the *Elastic Net* discussed in class, find an alternative model specification that having been trained using the 80% of the original sample has a *higher* proportion of true predictions for the validation set that your found in 4 above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of choosing a model, I will try lasso and ridge separating and continue with one method. First I include all the variables and their interaction terms in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I interact all the explanatory variables with race, sex and thickness of file to find variables that result in a higher true predictions than 88.2952%.\n",
    "# I find that 'LOC' lowers true predictions, so I exclude it from interaction.\n",
    "# I also discover that 'UEMP','SCH' and 'OBRAT' increase the proportion significantly while 'MHIST' decreases the proportion. Other variables do not significantly affect the proportion.\n",
    "# Thus, I remove 'MHIST' and 'LOC' from interaction terms and add 'UEMP' and 'OBRAT' as new interaction terms.\n",
    "\n",
    "# Create a new dataframe\n",
    "df1 = df_new[['APPROVE','HRAT','OBRAT','MHIST','PUB','SELF','CHIST','UEMP','MULTI','COSIGN','MS','LNPR','DEP','SCH','THK','ARAC','ASEX','LOC','SCHRAC','ACHSEX','SCHTHK','SCHELOC']].copy()\n",
    "\n",
    "# Create new interaction variables\n",
    "tmp1 = ['ARAC','ASEX','THK']\n",
    "for x in tmp1:\n",
    "    df1['UEMP'+x] = df1['UEMP']*df1[x]\n",
    "    df1['OBRAT'+x] = df1['OBRAT']*df1[x]\n",
    "    \n",
    "y = df1['APPROVE']\n",
    "# Create new predictor variables\n",
    "X1 = df1.drop('APPROVE',axis=1)\n",
    "\n",
    "# Partition the original data set into train (0.8) and test (0.2) data sets\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of true predictions for my validation set is 89.56743 %.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data before fitting and evaluating the model\n",
    "sc = StandardScaler()\n",
    "X1_train = sc.fit_transform(X1_train)\n",
    "X1_test = sc.fit_transform(X1_test)\n",
    "\n",
    "# Use 5-fold cross-validation on the train data set over a combination of 20 values of lambda and 10 values of alpha\n",
    "from sklearn.model_selection import KFold\n",
    "fold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "searchCV = LogisticRegressionCV(\n",
    "    Cs=list(np.linspace(0.1,2,20,endpoint=True)) #this corresponds to 1/lambda above\n",
    "    ,penalty='elasticnet'\n",
    "    ,l1_ratios=np.linspace(0.1,0.9,10,endpoint=True) #this corresponds to alpha above\n",
    "    ,scoring='accuracy' #proportion of main diag of confusion matrix\n",
    "    ,cv=fold\n",
    "    ,random_state=42\n",
    "    ,max_iter=10000\n",
    "    ,fit_intercept=True\n",
    "    ,solver='saga' #only optimizer available for elasticnet\n",
    "    ,tol=10\n",
    ")\n",
    "logit_cv1 = searchCV.fit(X1_train, y1_train.values.ravel())\n",
    "\n",
    "# Print the proportion of true predictions for my validation set\n",
    "proportion1 = logit_cv1.score(X1_test, y1_test)*100\n",
    "print(f'The proportion of true predictions for my validation set is {round(proportion1,6)} %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LASSO: \n",
    "\n",
    "This time, I want to check if a LASSO regression has a higher proportion than 89.5674%. If it does, I can select nonzero variables from LASSO for my model specification; otherwise I will keep my first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of all explanatory variables and remove all interaction terms\n",
    "df2 = df_new[['APPROVE','HRAT','OBRAT','MHIST','PUB','SELF','CHIST','UEMP','MULTI','COSIGN','MS','LNPR','DEP','SCH','THK','ARAC','ASEX','LOC']].copy()\n",
    "\n",
    "# Make demeaned variables for each\n",
    "tmp2 = ['OBRAT','HRAT','OBRAT','MHIST','PUB','SELF','CHIST','UEMP','MULTI','COSIGN','MS','LNPR','DEP','SCH','THK','ARAC','ASEX','LOC']\n",
    "for x in tmp2:\n",
    "    df2[x+'_dmean'] = df2[x]-df2[x].mean()\n",
    "\n",
    "# Make interaction terms for each\n",
    "tmp2str = []\n",
    "for x in range(len(tmp2)):\n",
    "    for y in tmp2[x+1:]:\n",
    "        if x != y:\n",
    "            tmp2str.append('('+tmp2[x]+'_dmean:'+y+'_dmean'+')')\n",
    "            \n",
    "# Make specifications\n",
    "f2 = 'APPROVE ~ -1 +' + ''.join([x+'+' for x in tmp2])[:-1] + ' + ' + ''.join([x+'+' for x in tmp2str])[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outcome vector and design matrices\n",
    "y2, X2 = patsy.dmatrices(f2, data=df2, return_type='dataframe')\n",
    "\n",
    "# Create the indices for the train (80%) and validation (20%) data sets.\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data before fitting and evaluating the model\n",
    "X2_train = sc.fit_transform(X2_train)\n",
    "X2_test = sc.fit_transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of true predictions for my validation set using LASSO alone is 88.5496 %.\n"
     ]
    }
   ],
   "source": [
    "# Use Lasso only\n",
    "\n",
    "lassoCV = LogisticRegressionCV(\n",
    "    penalty='l1'\n",
    "    ,scoring='accuracy' #proportion of main diag of confusion matrix\n",
    "    ,cv=fold\n",
    "    ,random_state = 42\n",
    "    ,max_iter=1000000\n",
    "    ,fit_intercept=True  \n",
    "    ,solver='saga'\n",
    ")\n",
    "\n",
    "logit2_lasso = lassoCV.fit(X2_train, y2_train.values.ravel())\n",
    "\n",
    "\n",
    "proportionLASSO = round(logit2_lasso.score(X2_test, y2_test),6)*100\n",
    "print(f'The proportion of true predictions for my validation set using LASSO alone is {proportionLASSO} %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the prediction has a lower proportion of accuracy than the first one, I will stop here. Next, I apply Ridge Regression to see if the prediction is more accurate than my first model specification.\n",
    "\n",
    "Using Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of true predictions for my validation set using Ridge alone is 87.5318 %.\n"
     ]
    }
   ],
   "source": [
    "# Use Ridge alone\n",
    "\n",
    "ridgeCV = LogisticRegressionCV(\n",
    "    penalty='l2'\n",
    "    ,scoring='accuracy' #proportion of main diag of confusion matrix\n",
    "    ,cv=fold\n",
    "    ,max_iter=1000000\n",
    "    ,fit_intercept=True  \n",
    "    ,solver='saga'\n",
    ")\n",
    "logit3_ridge = ridgeCV.fit(X2_train, y2_train.values.ravel())\n",
    "\n",
    "proportionRidge = round(logit3_ridge.score(X2_test, y2_test),6)*100\n",
    "print(f'The proportion of true predictions for my validation set using Ridge alone is {proportionRidge} %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Ridge has a lower proportion than LASSO, I will stop here. My first model obtains the highest proportion, so my alternative model specifiction is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HRAT', 'OBRAT', 'MHIST', 'PUB', 'SELF', 'CHIST', 'UEMP', 'MULTI', 'COSIGN', 'MS', 'LNPR', 'DEP', 'SCH', 'THK', 'ARAC', 'ASEX', 'LOC', 'SCHRAC', 'ACHSEX', 'SCHTHK', 'SCHELOC', 'UEMPARAC', 'OBRATARAC', 'UEMPASEX', 'OBRATASEX', 'UEMPTHK', 'OBRATTHK']\n",
      "The coefficients are [[ 0.16902022 -0.61264117  0.17605503  0.34487681 -0.26657145  0.53883369\n",
      "  -0.10191509 -0.08610914  0.00779537  0.13985363 -0.15989656  0.01250593\n",
      "  -0.16412423  0.00086282  0.35803503 -0.14358337  0.07889224 -0.10060816\n",
      "   0.09688106  0.07727682  0.13552818  0.01515798  0.10010431 -0.16205141\n",
      "  -0.15153927  0.21290882 -0.06117876]].\n",
      "The proportion of true predictions based on my best model is 89.56743 %.\n"
     ]
    }
   ],
   "source": [
    "print(list(X1.columns))\n",
    "print(f'The coefficients are {logit_cv1.coef_}.')\n",
    "print(f'The proportion of true predictions based on my best model is {round(proportion1,6)} %.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
